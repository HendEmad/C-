Its problem is like any search problem, but here there is a min player whose goal is to win the max player. So we should calcualte also the affect of min player's moves.

Certain assumptions exist: 
- A two player game.
- The play is sequential (agents taking turns).
- The opponent is playing perfectly.

The two players are referred to as MAX and MIN:
- MAX wants to maximize the utility.
- MIN wants to minimize the utility.

### Game Problem Formulation:
A game with 2 players (MAX & MIN, MAX moves first, turn-taking) can be defined as search problem with:
- ***Initial state [S0]***: board position.
- ***Player [S]***: Player to move.
- ***Successor function [ACTIONS]***: a list of legal (move, state) pairs.
- ***Goal test [TERMINAL-TEST]***: whether the game is over - terminal states.
- ***Utility function [COST]***: Gives a numeric value for the terminal states (win[max = 1], loss[max = -1], draw[max = 0] in case of win-loss game) --> The value is calculated according to the maximun(algorithm).

 ### Game tree:
 ![Capture](https://user-images.githubusercontent.com/91827137/170881780-788f5aae-c274-422c-b7dd-c109d06b418b.PNG)

- Max is the first player to move, starting at the root node.
- The utility at the leaf nodes are from the `MAX's point of view`.
- The optimal strategy is found by examining the minimax value of each node.
- In a normal search tree, a solution is to find a path from initial state to goal state.
- However, since MIN affects the search, a contingent strategy should be found.
- The strategy defines:
    * MAX's move at the initial state.
    * MAX's move at all states generated by all possible moves by MIN.
    * and so on...
- This means that MAX needs a winning strategy independent of what MIN does.

In conclusion: We create the initial state, then MAX adds all its posibilites then it adds all posiblities of its competitor which is MIN here.

### The algorithm explanation: 
It have `3 functions`:
- 1st function [UTILITY]--> Asks if the game is over or not (win, loss, draw) (terminal state), it considers all actions that it can take
- 2nd function --> if we don't reach the terminal state (the game is not over), it finds which player will be play, then get all actions of the player and calculates the min (from the max's point of view)
- 3rd function is like 2nd function but in case of it's max's turn 

![Capture](https://user-images.githubusercontent.com/91827137/170882763-5db3f14b-fb98-4add-9983-7e75435097b5.PNG)

example:

![Capture](https://user-images.githubusercontent.com/91827137/170883136-bb9f336f-7a58-42f4-9fc4-eb23deaf9c54.PNG)

### MINIMAX Algorithm:
![Capture](https://user-images.githubusercontent.com/91827137/170883329-145f6e58-e1eb-4212-b4cd-b280336d75f5.PNG)

***note***: the max will get the maximum value from the min returned values and vise versa.

The algorithm have two conditions: maximum & minimum 
- We will give it the initial state, then it will get all possible actions to take. To get the maximum, it will call the MAX_VALUE function. If MAX_VALUE function returns terminal state, the algorithm will returns the same value.
- If MAX_VALUE funcitons doesn't return terminal state, it will get the maximun value for each action from the min returnes values. so this function is recursive function, it calls another function which is MIN_VALUE funciton to take all values returned from it.
- The MIN_VALUE will work exactly as MAX_VALUE funciton but instead of getting the max of mimimum returned value (as in max_value function), it calculates the minimum values that are the result of the max_value funciton. So, it will call MAX_VALUE function.

***notes:***
- In MAX_VALUE function --> we set the initial value to -∞ to be always small compared to all values returned from min_value function, so the result be all values returned from the MIN_VALUE function.
- Regarding the initial value of v in MIN_VALUE function, what happens is the opposite of what happens in MAX_VALUE funciton.

### Advantages:
- Optimal against an optimal opponent.
- Space complexity = O(bm) --> depth-first exploration

### Disadvantages:
- It is exponential in time --> Time complexity = O(b^m)

* Game trees are too huge to be solved to optimality.
* The algorithm has to generate the entire tree, compute the utility values at the leaf nodes, and then proragate these values up the tree.
* We want a way to do less work (which leads to less time) by removing unnecessary branches thus exploring less nodes, so how can we know unnecessary branches?
* To do this: we will use the `Alpha-Beta pruning technique`.

### [α-β Pruning](https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-4-alpha-beta-pruning/):
It is an optimization technique for minimax algorithm. It reduces the computation time by a huge factor. This allows us to search much faster and even go into deeper levels in the game tree. It cuts off branches in the game tree which need not be searched because there already exists a better move available. It is called Alpha-Beta pruning because it passes 2 extra parameters in the minimax function, namely alpha and beta.
- `Alpha` is the best value that the ***maximizer*** currently can guarantee at that level or above. 
- `Beta` is the best value that the ***minimizer*** currently can guarantee at that level or above.
- if α >= β --> Cutoff

for the same exapme:
1. first step --> E node --> K --> continue:

![Capture](https://user-images.githubusercontent.com/91827137/170886590-72ab7a45-551a-458f-b2b4-2498de0b9ad8.PNG)

2. second step --> E node --> L --> contunie:

![Capture](https://user-images.githubusercontent.com/91827137/170886690-b80f8913-1807-4850-a669-a613c74a1117.PNG)

3. We finished E node (E-path), we will update β to 3(minimum value) and α will be -∞:

![Capture](https://user-images.githubusercontent.com/91827137/170886860-0942510e-cb16-40a3-b0d0-35dbd9f6f105.PNG)

4. 

![Capture](https://user-images.githubusercontent.com/91827137/170886969-92577ad7-c646-4539-a49f-f5c253a23b78.PNG)

5.

![Capture](https://user-images.githubusercontent.com/91827137/170887035-1bad8bf6-679d-4dc1-9a5d-86de791880bf.PNG)

6.

![Capture](https://user-images.githubusercontent.com/91827137/170887130-dde0b769-c351-4be8-af7d-aebe6eb01a85.PNG)

7.

![Capture](https://user-images.githubusercontent.com/91827137/170887168-924ff428-9007-44a9-8fc7-4a1a3e0069f9.PNG)

8.

![Capture](https://user-images.githubusercontent.com/91827137/170887200-68b1942e-00cf-42f7-89b6-2e2c6dcdaf40.PNG)

9.

![Capture](https://user-images.githubusercontent.com/91827137/170887228-4842f2e6-ff0a-443e-b013-90978b7196ca.PNG)

10.

![Capture](https://user-images.githubusercontent.com/91827137/170887246-cc83f23f-077c-4bdc-8401-f050d860c45c.PNG)

11.

![Capture](https://user-images.githubusercontent.com/91827137/170887274-2209a563-3761-41e8-8a71-1b45d6ee4438.PNG)

So, the new algorithm will be:

![Capture](https://user-images.githubusercontent.com/91827137/170887328-3c6c46f1-6ca7-47e2-b0c1-9e7231d5c51d.PNG)
